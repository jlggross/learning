{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Course - Merging DataFrames with pandas.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNLcg6cxZmHjIFlirOfe1PR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jsbffugJSv5Y","colab_type":"text"},"source":["# Course: Merging DataFrames with pandas \n","\n","* Link: https://learn.datacamp.com/courses/merging-dataframes-with-pandas\n","\n","* Datasets: https://www.data.gov/\n"]},{"cell_type":"code","metadata":{"id":"EGnrLja9TNEj","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# Importing weather data\n","# --------------------------------------------------------------------------------------------------------\n","import pandas as pd\n","w_mean = pd.read_csv('quarterly_mean_temp.csv', index_col='Month')\n","w_max = pd.read_csv('quarterly_max_temp.csv', index_col='Month')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lW2L1sOuT2ke","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# The DataFrame indexes\n","# --------------------------------------------------------------------------------------------------------\n","print(w_mean.index)\n","# > Index(['Apr', 'Jan', 'Jul', 'Oct'], dtype='object', name='Month')\n","\n","print(w_max.index)\n","# > Index(['Jan', 'Apr', 'Jul', 'Oct'], dtype='object', name='Month')\n","\n","print(type(w_mean.index))\n","# > <class 'pandas.indexes.base.Index'>\n","\n","# Rename column names\n","temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n","temps_c = temps_f.copy()\n","temps_c.columns = temps_c.columns.str.replace('F', 'C') # 'Min TemperatureC', 'Mean TemperatureC', 'Max TemperatureC'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFmvBm_lbAm0","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# Setting multiple indexes\n","# --------------------------------------------------------------------------------------------------------\n","names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1)) # Set as indexes \"name\" and \"gender\"\n","names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UkX64iwUnL4","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# Using .reindex() vs Using .sort_index()\n","# --------------------------------------------------------------------------------------------------------\n","ordered = ['Jan', 'Apr', 'Jul', 'Oct']\n","w_mean2 = w_mean.reindex(ordered) # Order by the specific order, according to the list\n","w_mean2.sort_index() # Order according to the values in the index\n","\n","# Using Dataframe Index\n","w_mean.reindex(w_max.index).dropna() # Order by the index of DataFrame w_max and drop rows that could have been created during the process\n","\n","# Sorting\n","weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n","weather2 = weather1.sort_index()\n","weather3 = weather1.sort_index(ascending=False)\n","weather4 = weather1.sort_values('Max TemperatureF')\n","\n","# Reindex and forward-fill\n","weather1 # Has just months 'Jan', 'Apr', 'Jul' and 'Oct'\n","year # Has all the months of the year\n","weather2 = weather1.reindex(year) # Will create 8 more rows, for those months that are not in weather1. New rows will get value NaN\n","weather3 = weather1.reindex(year).ffill() # Forward-fill cells with NaN value. Repeat the value of the antecedent row/cell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qoqUHUdUsFe","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# Arithmetic with Series & DataFrames\n","# --------------------------------------------------------------------------------------------------------\n","weather = pd.read_csv('pittsburgh2013.csv', index_col='Date', parse_dates=True) # With parse_dates=True we get datetime objects\n","\n","# Division\n","week1_range = weather.loc['2013-07-01':'2013-07-07', ['Min TemperatureF', 'Max TemperatureF']]\n","week1_mean = weather.loc['2013-07-01':'2013-07-07', 'Mean TemperatureF'] \n","week1_range.divide(week1_mean, axis='rows')\n","\n","# Percentage change\n","week1_mean.pct_change() * 100 \n","\n","# Addition\n","bronze = pd.read_csv('bronze_top5.csv', index_col=0)\n","silver = pd.read_csv('silver_top5.csv', index_col=0)\n","gold = pd.read_csv('gold_top5.csv', index_col=0)\n","bronze.add(silver, fill_value=0) # Add two dataframes. fill_value=0 to avoid NaN values\n","bronze.add(silver, fill_value=0).add(gold, fill_value=0) # Add three datagrames\n","\n","# Example 1\n","gdp = pd.read_csv('GDP.csv', index_col='DATE', parse_dates=True)\n","post2008 = gdp.loc['2008':,:] # Slice all the gdp data from 2008 onward\n","print(post2008.tail(8)) # Print the last 8 rows of post2008\n","yearly = post2008.resample('A').last() # Gets sample from annual frequency ('A') and get the last element of each year\n","yearly['growth'] = yearly.pct_change() * 100 # Compute percentage growth of yearly: yearly['growth']\n","\n","# Example 2\n","sp500 = pd.read_csv('sp500.csv')\n","exchange = pd.read_csv('exchange.csv')\n","dollars = sp500[['Open', 'Close']] # Subset 'Open' & 'Close' columns\n","print(dollars.head())\n","pounds = dollars.multiply(exchange['GBP/USD'], axis='rows') # Convert dollars to pounds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZSRfaMDiR2g","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------\n","# Appending & concatenating Series\n","# --------------------------------------------------------------------------------------------------------\n","result1 = pd.concat([s1, s2, s3])\n","result2 = s1.append(s2).append(s3)\n","result1 == result2 elementwise\n","\n","import pandas as pd\n","northeast = pd.Series(['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'])\n","south = pd.Series(['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'])\n","midwest = pd.Series(['IL', 'IN', 'MN', 'MO', 'NE', 'ND', 'SD', 'IA', 'KS', 'MI', 'OH', 'WI'])\n","west = pd.Series(['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR','WA'])\n","\n","# Append \n","east = northeast.append(south) # Has repeated indeces\n","new_east = northeast.append(south).reset_index(drop=True) # Create new index without any repeated indice\n","\n","# Concat\n","east = pd.concat([northeast, south]) # Has repeated indices\n","new_east = pd.concat([northeast, south], ignore_index=True) # Create new index without repeated indices\n","new_east = pd.concat([northeast, south], axis=0) # Concatenate rows. axis=0 to concatenated 'south' in the bottom of 'northeast'\n","new_east = pd.concat([northeast, south], axis=1) # Concatenate columns. Equal indices create just one row in the final table\n","\n","# Example\n","medal = []\n","gold = pd.read_csv('gold.csv', header=0, index_col='Country', names=columns)\n","silver = pd.read_csv('silver.csv', header=0, index_col='Country', names=columns)\n","medals.append(gold).append(silver) # Create list with gold and silver\n","medals_df = pd.concat(medals, axis='columns') # Concatenate medals horizontally: medals_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdm6TVP9XlDy","colab_type":"code","colab":{}},"source":["# Concatenation, keys, & MultiIndexes\n","rain2013 = pd.read_csv(\"railfall2013.csv\", index_col='Month', parse_dates=True) \n","rain2014 = pd.read_csv(\"railfall2014.csv\", index_col='Month', parse_dates=True) \n","\n","rain1314 = pd.concat([rain2013, rain2014], axis=0) # Create repeated indeces, but each month is from a different year\n","rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis=0) # Creates an outer indix level with the year associated with each month\n","print(rain1314.loc[2014]) # Access results from year 2014\n","\n","rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis='columns') # Created multi-index in column level\n","\n","# Example 2\n","bronze = pd.read_csv('bronze.csv', index_col='Country')\n","silver = pd.read_csv('silver.csv', index_col='Country')\n","gold = pd.read_csv('gold.csv', index_col='Country')\n","\n","medals.append(gold).append(silver).append(silver) \n","medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n","medals_sorted = medals.sort_index(level=0) # Sort the entries of medals: medals_sorted\n","\n","print(medals_sorted.loc[('bronze','Germany')]) # Print the number of Bronze medals won by Germany\n","print(medals_sorted.loc['silver']) # Print data about silver medals\n","idx = pd.IndexSlice # Create alias 'idex' for pd.IndexSlice. A slicer pd.IndexSlice is required when slicing on the inner level of a MultiIndex.\n","print(medals_sorted.loc[idx[:,'United Kingdom'], :]) # Print all the data on medals won by the United Kingdom\n","\n","# Example 2\n","month_list = [('january', jan), ('february', feb), ('march', mar)]\n","month_dict = {} # Create dictionary\n","for month_name, month_data in month_list:\n","    month_dict[month_name] = month_data.groupby('Company').sum() # Group month_data by 'Company'\n","sales = pd.concat(month_dict) # Concatenate data in month_dict: sales\n","idx = pd.IndexSlice \n","print(sales.loc[idx[:, 'Mediacore'], :]) # Print all sales by Mediacore\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKy_uwrZLq3z","colab_type":"text"},"source":["# Joining tables: Combining rows of multiple tables\n","\n","Outer join\n","* **Union** of index sets (all labels, no repetition)\n","* Missing fields filled with NaN\n","\n","Inner join\n","* **Intersection** of index sets (only common labels)"]},{"cell_type":"code","metadata":{"id":"_QxwnINtLDRQ","colab_type":"code","colab":{}},"source":["# Outer & inner joins\n","\n","pd.concat([population, unemployment], axis=1, join='inner') # Has just the columns of both sets\n","pd.concat([population, unemployment], axis=1, join='outer') # Has the columns of all sets\n","\n","#Example\n","china_annual = china.resample('A').last().pct_change(10).dropna()\n","us_annual = us.resample('A').last().pct_change(10).dropna()\n","gdp = pd.concat([china_annual, us_annual], join='inner', axis=1) # Concatenate china_annual and us_annual\n","print(gdp.resample('10A').last()) # Resample 10 years (annual) gdp and print"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1pmVnhvlL5FE","colab_type":"code","colab":{}},"source":["# Merging DataFrames\n","\n","pd.merge(counties, cities, left_on='CITY NAME', right_on='City')\n","'''Works like a inner join, merging only the columns that occur \n","in both DataFrames. A new row is made for each entry that is in\n","the columns of both DataFrames, merging all other columns.'''\n","\n","pd.merge(bronze, gold, on='NOC')\n","'''Both DataFrames have exactly the same columns. So a columns is \n","specified to be examinated for the merge ('NOC'). Columns 'country' is \n","repeated in the final merge. '''\n","\n","pd.merge(bronze, gold, on=['NOC', 'Country']) \n","'''Merge both DataFrames by columns 'NOC' and 'Contry'. The result is\n","a DataFrame with columns 'NOC', 'Country', 'Total_x' and 'Total_y' '''\n","\n","pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'])\n","'''Changes sufixes 'Total_x' and 'Total_y' for 'Total_bronze'\n","and 'Total_gold' '''\n","\n","pd.merge(counties, cities, left_on='CITY NAME', right_on='City')\n","'''Merge DataFrames with different columns names. Column names are\n","specified. left_on for DataFrame 'counties' and right_on for DataFrame\n","'cities'. '''\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9ep0d2Swm5b","colab_type":"text"},"source":["# Joining DataFrames\n","\n","Merging with left join\n","* Keeps all rows of the left DF in the merged DF\n","* For rows in the left DF with matches in the right DF:\n","> Non-joining columns of right DF are appended to left DF\n","* For rows in the le! DF with no matches in the right DF:\n","> Non-joining columns are filled with nulls\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pm8lKuxn6CEp","colab_type":"code","colab":{}},"source":["# Ordered merges\n","\n","pd.merge(hardware, software, how='outer').sorted_values('Date') \n","pd.merge_ordered(hardware, software) # Does a merge outer join and order values\n","\n","pd.merge_ordered(hardware, software, on=['Date', 'Company'],\n","...: suffixes=['_hardware', '_software'])\n","\n","pd.merge_ordered(hardware, software, on=['Date', 'Company'],\n","...: suffixes=['_hardware', '_software'], fill_method='ffill') # Add forward-fill"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1iJXGCGQmFxc","colab_type":"text"},"source":["Course Final Example"]},{"cell_type":"code","metadata":{"id":"Jy3hXaXbmREA","colab_type":"code","colab":{}},"source":["# ----------------------------------------------------------------------\n","# Creating DataFrames of Summer Olympics Medals\n","# ----------------------------------------------------------------------\n","import pandas as pd\n","\n","file_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n","editions = pd.read_csv(file_path, sep='\\t')\n","editions = editions[['Edition', 'Grand Total', 'City', 'Country']] # Extract the relevant columns\n","print(editions)\n","\n","file_path = 'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'\n","ioc_codes = pd.read_csv(file_path) # Load DataFrame from file_path\n","ioc_codes = ioc_codes[['Country', 'NOC']] # Extract the relevant columns\n","print(ioc_codes.head())\n","print(ioc_codes.tail())\n","\n","medals_dict = {} # Create empty dictionary\n","for year in editions['Edition']:\n","    file_path = 'summer_{:d}.csv'.format(year)  \n","    medals_dict[year] = pd.read_csv(file_path) # Load file_path into a DataFrame\n","    medals_dict[year] = medals_dict[year][['Athlete', 'NOC', 'Medal']] # Extract relevant columns    \n","    medals_dict[year]['Edition'] = year # Assign year to column 'Edition'\n","medals = pd.concat(medals_dict, ignore_index=True) # Concatenate medals_dict\n","print(medals.head())\n","print(medals.tail())\n","\n","# ----------------------------------------------------------------------\n","# Extracting Quantitative Measurements\n","# ----------------------------------------------------------------------\n","import pandas as pd \n","\n","medal_counts = pd.pivot_table(medals, aggfunc='count', index='Edition', values='Athlete', columns='NOC')\n","print(medal_counts.head())\n","print(medal_counts.tail())\n","\n","totals = editions.set_index('Edition') # Set Index of editions\n","totals = totals['Grand Total'] # Reassign totals['Grand Total']\n","fractions = medal_counts.divide(totals, axis='rows') # Divide medal_counts by totals\n","print(fractions.head())\n","print(fractions.tail())\n","\n","mean_fractions = fractions.expanding().mean() # Apply the expanding mean\n","fractions_change = mean_fractions.pct_change()*100 # Compute the percentage change\n","fractions_change = fractions_change.reset_index('Edition') # Reset the index of fractions_change\n","print(fractions_change.head())\n","print(fractions_change.tail())\n","\n","# ----------------------------------------------------------------------\n","# Reshaping and Plotting\n","# ----------------------------------------------------------------------\n","import pandas as pd\n","\n","hosts = pd.merge(editions, ioc_codes, how='left') # Left join editions and ioc_codes\n","hosts = hosts[['Edition', 'NOC']].set_index('Edition') # Extract relevant columns and set index\n","print(hosts.loc[hosts.NOC.isnull()]) # Fix missing 'NOC' values of hosts\n","hosts.loc[1972, 'NOC'] = 'FRG'\n","hosts.loc[1980, 'NOC'] = 'URS'\n","hosts.loc[1988, 'NOC'] = 'KOR'\n","hosts = hosts.reset_index() # Reset Index of hosts\n","print(hosts)\n","\n","reshaped = pd.melt(fractions_change, id_vars='Edition', value_name='Change') # Reshape fractions_change\n","print(reshaped.shape, fractions_change.shape)\n","chn = reshaped[reshaped['NOC'] == 'CHN'] # Extract rows from reshaped where 'NOC' == 'CHN'\n","print(chn.tail())\n","\n","'''\n","Conclusion: On looking at the hosting countries from the last 5 Olympic \n","editions and the fractional change of medals won by China the last 5 \n","editions, you can see that China fared significantly better in 2008\n","(i.e., when China was the host country).\n","'''\n","\n","# Merging to compute influence\n","merged = pd.merge(reshaped, hosts, how='inner') \n","print(merged.head())\n","influence = merged.set_index('Edition').sort_index()\n","print(influence.head())\n","\n","import matplotlib.pyplot as plt\n","change = influence['Change']\n","ax = change.plot(kind='bar') # Bar plot\n","\n","# Customize the plot to improve readability\n","ax.set_ylabel(\"% Change of Host Country Medal Count\")\n","ax.set_title(\"Is there a Host Country Advantage?\")\n","ax.set_xticklabels(editions['City'])\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}